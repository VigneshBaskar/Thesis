Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rong2014,
abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been proven to be able to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec, I notice that there lacks a material that comprehensively explains the parameter learning process of word2vec in details, thus preventing many people with less neural network experience from understanding how exactly word2vec works. This note provides detailed derivations and explanations of the parameter update equations for the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram models, as well as advanced tricks, hierarchical soft-max and negative sampling. In the appendix a review is given on the basics of neuron network models and backpropagation.},
archivePrefix = {arXiv},
arxivId = {1411.2738},
author = {Rong, Xin},
eprint = {1411.2738},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Word 2 Vector Articles/Word2Vec parameter learning.pdf:pdf},
journal = {arXiv:1411.2738},
pages = {1--19},
title = {{word2vec Parameter Learning Explained}},
url = {http://arxiv.org/abs/1411.2738},
year = {2014}
}
@article{Do2008,
abstract = {The expectation maximization algorithm arises in many computational biology applications that involve probabilistic models. What is it good for, and how does it work?},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Do, Chuong B and Batzoglou, Serafim},
doi = {10.1038/nbt1406},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/EM Tutorial 2.pdf:pdf},
isbn = {1546-1696},
issn = {1087-0156},
journal = {Nature biotechnology},
number = {8},
pages = {897--899},
pmid = {18688245},
title = {{What is the expectation maximization algorithm?}},
volume = {26},
year = {2008}
}
@article{Cremonesi2010,
abstract = {In many commercial systems, the ‘best bet' recommenda- tions are shown, but the predicted rating values are not. This is usually referred to as a top-N recommendation task, where the goal of the recommender system is to find a few specific items which are supposed to be most appealing to the user. Common methodologies based on error metrics (such as RMSE) are not a natural fit for evaluating the top- N recommendation task. Rather, top-N performance can be directly measured by alternative methodologies based on accuracy metrics (such as precision/recall). An extensive evaluation of several state-of-the art recom- mender algorithms suggests that algorithms optimized for minimizing RMSE do not necessarily perform as expected in terms of top-N recommendation task. Results show that improvements in RMSE often do not translate into accu- racy improvements. In particular, a naive non-personalized algorithm can outperform some common recommendation approaches and almost match the accuracy of sophisticated algorithms. Another finding is that the very few top popular items can skew the top-N performance. The analysis points out that when evaluating a recommender algorithm on the top-N recommendation task, the test set should be chosen carefully in order to not bias accuracy metrics towards non- personalized solutions. Finally, we offer practitioners new variants of two collaborative filtering algorithms that, re- gardless of their RMSE, significantly outperform other rec- ommender algorithms in pursuing the top-N recommenda- tion task, with offering additional practical advantages. This comes at surprise given the simplicity of these two methods.},
author = {Cremonesi, Paolo and Koren, Yehuda and Turrin, Roberto},
doi = {10.1145/1864708.1864721},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cremonesi, Koren, Turrin - 2010 - Performance of recommender algorithms on top-n recommendation tasks.pdf:pdf},
isbn = {9781605589060},
issn = {10468188},
journal = {Proceedings of the fourth ACM conference on Recommender systems - RecSys '10},
keywords = {evaluation,precision,recall,top-n recommendations},
number = {February},
pages = {39},
title = {{Performance of recommender algorithms on top-n recommendation tasks}},
url = {http://portal.acm.org/citation.cfm?doid=1864708.1864721},
year = {2010}
}
@article{Paterek2007,
abstract = {A key part of a recommender system is a collaborative filter- ing algorithm predicting users' preferences for items. In this paper we describe different efficient collaborative filtering techniques and a framework for combining them to obtain a good prediction. The methods described in this paper are the most im- portant parts of a solution predicting users' preferences for movies with error rate 7.04{\%} better on the Netflix Prize dataset than the reference algorithm Netflix Cinematch. The set of predictors used includes algorithms suggested by Netflix Prize contestants: regularized singular value de- composition of data with missing values, K-means, postpro- cessing SVD with KNN. We propose extending the set of predictors with the following methods: addition of biases to the regularized SVD, postprocessing SVD with kernel ridge regression, using a separate linear model for each movie, and using methods similar to the regularized SVD, but with fewer parameters. All predictors and selected 2-way interactions between them are combined using linear regression on a holdout set.},
author = {Paterek, Arkadiusz},
doi = {10.1145/1557019.1557072},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Static Model Based Recommender Systems/SVD Collaborative Filtering.pdf:pdf},
isbn = {9781595938343},
issn = {00121606},
journal = {KDD Cup and Workshop},
keywords = {Netflix Prize,collaborative filtering,prediction,recommender systems},
pages = {2--5},
pmid = {9659936},
title = {{Improving regularized singular value decomposition for collaborative filtering}},
year = {2007}
}
@article{Takacs2008,
abstract = {Matrix factorization (MF) based approaches have proven to be efficient for rating-based recommendation systems. In this work, we propose several matrix factorization approaches with improved prediction accuracy. We introduce a novel and fast (semi)-positive MF approach that approximates the features by using positive values for either users or items. We describe a momentum-based MF approach. A transductive version of MF is also introduced, which uses information from test instances (namely the ratings users have given for certain items) to improve prediction accuracy. We describe an incremental variant of MF that efficiently handles new users/ratings, which is crucial in a real-life recommender system. A hybrid MF--neighbor-based method is also discussed that further improves the performance of MF.The proposed methods are evaluated on the Netflix Prize dataset, and we show that they can achieve very favorable Quiz RMSE (best single method: 0.8904, combination: 0.8841) and running time.},
author = {Tak{\'{a}}cs, G{\'{a}}bor and Pil{\'{a}}szy, Istv{\'{a}}n and N{\'{e}}meth, Botty{\'{a}}n and Tikk, Domonkos and Pilaszy, I and N'emeth, B and Tikk, Domonkos},
doi = {10.1109/ICDMW.2008.86},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Static Model Based Recommender Systems/Matrix Factorization Methods.pdf:pdf},
isbn = {9781605582658},
journal = {Proceedings of the 2nd KDD Workshop on Large Scale Recommender Systems and the Netflix Prize Competition},
keywords = {collaborative filtering,gradient descent methods,incremental,matrix factorization,neighbor-based methods,netflix prize,recommender systems},
pages = {553--562},
title = {{Investigation of Various Matrix Factorization Methods for Large Recommender Systems}},
volume = {1},
year = {2008}
}
@article{Mikolov2013b,
abstract = {Continuous space language models have re-cently demonstrated outstanding results across a variety of tasks. In this paper, we ex-amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, " King -Man + Woman " results in a vector very close to " Queen. " We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu-larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re-markably, this method outperforms the best previous systems.},
author = {Mikolov, Tomas and Yih, Wen-Tau and Zweig, Geoffrey},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Word 2 Vector Articles/Linguistic Regularities in Continuous Space Word Representations.pdf:pdf},
isbn = {9781937284473},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
year = {2013}
}
@article{Bilmes1998,
abstract = {We describe the maximum-likelihood parameter estimation problem and howthe Expectation- Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EMalgorithm as it is often given in the literature. We then develop the EM parameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any convergence properties. We try to emphasize intuition rather than mathematical rigor},
author = {Bilmes, Jeff},
doi = {10.1.1.28.613},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/HMM and EM.pdf:pdf},
title = {{A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.613},
year = {1998}
}
@article{Cremonesi2010,
abstract = {In many commercial systems, the ‘best bet' recommenda- tions are shown, but the predicted rating values are not. This is usually referred to as a top-N recommendation task, where the goal of the recommender system is to find a few specific items which are supposed to be most appealing to the user. Common methodologies based on error metrics (such as RMSE) are not a natural fit for evaluating the top- N recommendation task. Rather, top-N performance can be directly measured by alternative methodologies based on accuracy metrics (such as precision/recall). An extensive evaluation of several state-of-the art recom- mender algorithms suggests that algorithms optimized for minimizing RMSE do not necessarily perform as expected in terms of top-N recommendation task. Results show that improvements in RMSE often do not translate into accu- racy improvements. In particular, a naive non-personalized algorithm can outperform some common recommendation approaches and almost match the accuracy of sophisticated algorithms. Another finding is that the very few top popular items can skew the top-N performance. The analysis points out that when evaluating a recommender algorithm on the top-N recommendation task, the test set should be chosen carefully in order to not bias accuracy metrics towards non- personalized solutions. Finally, we offer practitioners new variants of two collaborative filtering algorithms that, re- gardless of their RMSE, significantly outperform other rec- ommender algorithms in pursuing the top-N recommenda- tion task, with offering additional practical advantages. This comes at surprise given the simplicity of these two methods.},
author = {Cremonesi, Paolo and Koren, Yehuda and Turrin, Roberto},
doi = {10.1145/1864708.1864721},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cremonesi, Koren, Turrin - 2010 - Performance of recommender algorithms on top-n recommendation tasks.pdf:pdf},
isbn = {9781605589060},
issn = {10468188},
journal = {Proceedings of the fourth ACM conference on Recommender systems - RecSys '10},
keywords = {evaluation,precision,recall,top-n recommendations},
number = {February},
pages = {39},
title = {{Performance of recommender algorithms on top-n recommendation tasks}},
url = {http://portal.acm.org/citation.cfm?doid=1864708.1864721},
year = {2010}
}
@article{Wang2013,
author = {Wang, Ke},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2013 - Recommendation Systems.pdf:pdf},
title = {{Recommendation Systems}},
year = {2013}
}
@article{Yang,
abstract = {“Matrix decomposition refers to the transformation of a given matrix into a given canonical form.” [1], when the given matrix is transformed to a right-hand-side product of canonical matrices the process of producing this decomposition is also called “matrix factorization”. Matrix decomposition is a fundamen- tal theme in linear algebra and applied statistics which has both scientific and engineering significance. The purposes of matrix decomposition typically involve two aspects: computational convenience and an- alytic simplicity. In the real world, it is not feasible for most of the matrix computations to be calculated in an optimal explicit way, such as matrix inversion, matrix determinant, solving linear system and least square fitting, thus to convert a difficult matrix computation problem into several easier tasks such as solving triangular or diagonal system will greatly facilitate the calculations. Data matrices representing some numerical observations such as proximity matrix or correlation matrix are often huge and hard to analyze, therefore to decompose the data matrices into some lower-order or lower-rank canonical forms will reveal the inherent characteristic and structure of the matrices and help to interpret their meaning readily. This tutorial is primarily a summary of important matrix decomposition methods, we will first present some basic concepts in Section 2 and then introduce several fundamental matrix decomposition methods in the successive sections, e.g. SVD, LU, QR and Eigen decomposition. A unified view of matrix fac- torization derived from theWedderburn rank-one reduction theorem is briefly discussed in the summary Section 7.},
author = {Yang, Ming},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Matrix Decomposition.pdf:pdf},
journal = {Notes},
pages = {1--17},
title = {{Matrix Decomposition}}
}
@article{Kodratoff2014,
author = {Kodratoff, Y},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/HMM.pdf:pdf},
isbn = {9780080509303},
title = {{Introduction to Machine Learning}},
url = {https://books.google.co.il/books?id=AQyjBQAAQBAJ},
year = {2014}
}
@article{Chen2015,
abstract = {In recent years, a variety of review-based recommender systems have been developed, with the goal of incorporating the valuable information in user-generated textual reviews into the user modeling and recommending process. Advanced text analysis and opinion mining tech-niques enable the extraction of various types of review elements, such as the discussed topics, the multi-faceted nature of opinions, contextual information, comparative opinions, and reviewers' emotions. In this article, we provide a comprehensive overview of how the review elements have been exploited to improve standard content-based recommending, collaborative filtering, and preference-based product ranking techniques. The review-based recommender system's ability to alleviate the well-known rating sparsity and cold-start problems is emphasized. This survey classifies state-of-the-art studies into two principal branches: review-based user profile building and review-based product profile building. In the user profile sub-branch, the reviews are not only used to create term-based profiles, but also to infer or enhance ratings. Multi-faceted opinions can further be exploited to derive the weight/value preferences that users place on particular features. In another sub-branch, the product profile can be enriched with feature opinions or comparative opinions to better reflect its assessment quality. The merit of each branch of work is discussed in terms of both algorithm development and the way in which the proposed algo-rithms are evaluated. In addition, we discuss several future trends based on the survey, which may inspire investigators to pursue additional studies in this area.},
author = {Chen, Li and Chen, Guanliang and Wang, Feng},
doi = {10.1007/s11257-015-9155-5},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Review articles/Review of Recommender Systems.pdf:pdf},
issn = {15731391},
journal = {User Modeling and User-Adapted Interaction},
keywords = {Collaborative filtering {\textperiodcentered},Content-based recommending {\textperiodcentered},Opinion mining {\textperiodcentered},Preference-based product ranking,Product profile building {\textperiodcentered},Recommender systems {\textperiodcentered},Text analysis {\textperiodcentered},User profile building {\textperiodcentered},User reviews {\textperiodcentered}},
number = {2},
pages = {99--154},
title = {{Recommender Systems Based on User Reviews: The State of the Art}},
volume = {25},
year = {2015}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Salakhutdinov2007,
abstract = {Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the ProbabilisticMatrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a con- strained version of the PMF model that is based on the assumption that users who have rated similar sets ofmovies are likely to have similar preferences. The result- ingmodel is able to generalize considerably better for userswith very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7{\%} better than the score of Netflix's own system. 1},
author = {Salakhutdinov, R and Mnih, A},
doi = {10.1145/1390156.1390267},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Static Model Based Recommender Systems/Probabilistic Matrix Factorization for Recommender System.pdf:pdf},
isbn = {9781605582054},
issn = {1049-5258},
journal = {Proc. Advances in Neural Information Processing Systems 20 (NIPS 07)},
pages = {1257--1264},
title = {{Probabilistic Matrix Factorization.}},
url = {http://discovery.ucl.ac.uk/63248/},
year = {2007}
}
@article{Mikolov2013a,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Mikolov2013a,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Models1998,
abstract = {We describe the maximum-likelihood parameter estimation problem and howthe Expectation- Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EM algorithm as it is often given in the literature. We then develop the EM pa- rameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any conver- gence properties. We try to emphasize intuition rather than mathematical rigor},
author = {Models, Hidden Markov},
doi = {10.1016/S0550-3213(97)00753-0},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/EM Tutorial.pdf:pdf},
issn = {05503213},
journal = {ReCALL},
number = {510},
pages = {126},
title = {{A Gentle Tutorial of the EM Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.4498{\&}rep=rep1{\&}type=pdf},
volume = {1198},
year = {1998}
}
@article{Takacs2008a,
author = {Takacs, Gabor},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Static Model Based Recommender Systems/On the Gravity Recommender System Tutorial.pdf:pdf},
journal = {Technology},
pages = {1--46},
title = {{On the Gravity Recommendation System Presented by Matt Rodriguez Matrix Factorization Neighbor based approaches Co-Clustering of movies and users}},
year = {2008}
}
@article{Sahoo2010,
author = {Sahoo, Nachiketa and Singh, Param Vir and Mukhopadhyay, Tridas},
doi = {10.2139/ssrn.1700585},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/DynamicCollaborativeFiltering.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{A Hidden Markov Model for Collaborative Filtering}},
url = {http://www.ssrn.com/abstract=1700585},
year = {2010}
}
@article{Dunlavy2011,
abstract = {The data in many disciplines such as social networks, web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this paper, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T+1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T+2, T+3, etc.? In this paper, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links. We present a weight-based method for collapsing multi-year data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.},
archivePrefix = {arXiv},
arxivId = {1005.4006},
author = {Dunlavy, Daniel M. and Kolda, Tamara G. and Acar, Evrim},
doi = {10.1145/1921632.1921636},
eprint = {1005.4006},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Temporal Link Prediction using Matrix and Tensor.pdf:pdf},
isbn = {1556-4681},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
number = {2},
pages = {1--27},
title = {{Temporal Link Prediction using Matrix and Tensor Factorizations}},
url = {http://arxiv.org/abs/1005.4006},
volume = {5},
year = {2011}
}
@article{Koren2010,
author = {Koren, By Yehuda},
doi = {10.1145/1721654.1721677},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Colloborative filtering with temporal dynamics.pdf:pdf},
isbn = {9781605584959},
issn = {0001-0782},
journal = {Communications of the ACM},
keywords = {collaborative filtering,concept drift,recommender systems},
number = {4},
pages = {89--97},
title = {{Collaborative Filtering with Temporal Dynamics}},
volume = {53},
year = {2010}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/vigne/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Das2007,
author = {Das, Abhinandan},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Google News Recommender System.pdf:pdf},
isbn = {9781595936547},
keywords = {google news,mapreduce,mendation system,minhash,online recom-,plsi,scalable collaborative filtering},
pages = {271--280},
title = {{Google News Personalization : Scalable Online}},
year = {2007}
}
@article{Bobadilla2013,
abstract = {Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Bobadilla, J. and Ortega, F. and Hernando, A. and Guti??rrez, A.},
doi = {10.1016/j.knosys.2013.03.012},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Review articles/Recommender Systems Survey Bobadilla 2013.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cold-start,Collaborative filtering,Evaluation metrics,Hybrid,Internet of things,Prediction,Recommendation,Recommender systems,Similarity measures,Social},
pages = {109--132},
publisher = {Elsevier B.V.},
title = {{Recommender systems survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2013.03.012},
volume = {46},
year = {2013}
}
@article{Tikk2007,
abstract = {The Netflix Prize is a collaborative filtering problem. This subfield of machine learning has become popular from the late 1990s with the spread of online services that use rec- ommendation systems, such as e.g. Amazon, Yahoo! Mu- sic, and of course Netflix. The aim of such a system is to predict what items a user might like based on his/her and other users previous ratings. The dataset of Netflix Prize is much larger than the previously known benchmark sets, therefore we first show in the paper how to store it efficiently and adaptively to various algorithms. Then we describe the outline of our solution, called the Gravity Recommendation System (GRS), to the Netflix Prize contest, which is in the leader position with RMSE 0.8808 at the time of the submis- sion of the paper. GRS comprises of the combination of dif- ferent approaches that are presented in the main part of the paper. We then compare the effectiveness of some selected individual and combined approaches against a designated subset of the dataset, and discuss their important features and drawbacks. Beside the description of successful experi- ments we also report on the useful lessons of (temporarily) failed ideas and algorithms.},
author = {Tikk, Domonkos},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Static Model Based Recommender Systems/On the Gravity Recommender System.pdf:pdf},
journal = {Processing},
keywords = {collaborative filtering,ma,machine learning,netflix prize},
pages = {22--30},
title = {{On the Gravity Recommendation System}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:On+the+Gravity+Recommendation+System{\#}0},
volume = {2007},
year = {2007}
}
@article{Breiman2001,
abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
archivePrefix = {arXiv},
arxivId = {0010},
author = {Breiman, Leo},
doi = {10.2307/2676681},
eprint = {0010},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/The two cultures of Statistical Modelling.pdf:pdf},
isbn = {08834237},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {199--215},
pmid = {10511666},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://www.jstor.org/stable/2676681},
volume = {16},
year = {2001}
}
@article{Hernando2016,
abstract = {In this paper we present a novel technique for predicting the tastes of users in recommender systems based on collaborative filtering. Our technique is based on factorizing the rating matrix into two non negative matrices whose components lie within the range [0, 1] with an understandable probabilistic meaning. Thanks to this decomposition we can accurately predict the ratings of users, find out some groups of users with the same tastes, as well as justify and understand the recommendations our technique provides.},
author = {Hernando, Antonio and Bobadilla, Jes{\'{u}}s and Ortega, Fernando},
doi = {10.1016/j.knosys.2015.12.018},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Non Negative Matrix Factorization for collaborative filtering using Bayesian.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Collaborative filtering,Graphical probabilistic models,Matrix factorization,Recommender systems},
pages = {188--202},
title = {{A non negative matrix factorization for collaborative filtering recommender systems based on a Bayesian probabilistic model}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115005006},
volume = {97},
year = {2016}
}
@article{Xiong2010,
abstract = {Real-world relational data are seldom stationary, yet traditional collaborative filtering algorithms generally rely on this assumption. Motivated by our sales prediction problem, we propose a factor-based algorithm that is able to take time into account. By introducing additional factors for time, we formalize this problem as a tensor factorization with a special constraint on the time dimension. Further, we provide a fully Bayesian treatment to avoid tuning parameters and achieve automatic model complexity control. To learn the model we develop an efficient sampling procedure that is capable of analyzing large-scale data sets. This new algorithm, called Bayesian Probabilistic Tensor Factorization (BPTF), is evaluated on several real-world problems including sales prediction and movie recommendation. Empirical results demonstrate the superiority of our temporal model},
author = {Xiong, Liang and Chen, Xi and Huang, Tzu-kuo and Schneider, Jeff and Carbonell, Jaime G},
doi = {10.1137/1.9781611972801.19},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Temporal Collaborative Filtering with Bayesian Probabilistic Tens.pdf:pdf},
isbn = {9781611972801},
journal = {Proceedings of the SIAM International Conference on Data Mining},
pages = {211----222},
title = {{Temporal Collaborative Filtering with Bayesian Probabilistic Tensor Factorization}},
year = {2010}
}
@article{Adomavicius2005,
abstract = { This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
archivePrefix = {arXiv},
arxivId = {3},
author = {Adomavicius, Gediminas and Tuzhilin, Alexander},
doi = {10.1109/TKDE.2005.99},
eprint = {3},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Review articles/Review of Recommender Systems Massive Citation.pdf:pdf},
isbn = {1066100802},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Collaborative filtering,Extensions to recommander systems,Rating estimation methods,Recommander systems},
number = {6},
pages = {734--749},
pmid = {1423975},
title = {{Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions}},
volume = {17},
year = {2005}
}
@article{Wang2013,
author = {Wang, Ke},
title = {{Recommendation Systems}},
year = {2013}
}
@article{Moss2008,
abstract = {A 24-item rating scale was developed to assess student performance during tutorial sessions in problem-based learning (PBL) as conducted during the pre-clinical years of Medical School at the National Autonomous University of Mexico. Items were divided into 3 categories: independent study, group interaction, and reasoning skills. Fourteen tutors assessed 152 1st- and 2nd-yr students (mean age 19 yrs) in 16 tutorial groups. An exploratory factor analysis with an Oblimin rotation was carried out to identify the underlying dimensions of the questionnaire. Factor analysis yielded 4 factors (independent study, group interaction, reasoning skills, and active participation) that together accounted for 76.6{\%} of the variance. Their Cronbach reliability coefficients were 0.95, 0.83, 0.94, and 0.93, respectively, and 0.96 for the scale as a whole. Results suggest that the questionnaire provides a reliable identification of the fundamental components of the PBL method as observable in tutorial groups and could be a useful assessment instrument for tutors wishing to monitor students' progress in each of these components. (PsycINFO Database Record (c) 2010 APA, all rights reserved).},
author = {Moss, Larry},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Markov Model Based Recommender systems/Example Baum Welch.pdf:pdf},
journal = {Spring},
number = {11},
pages = {4--9},
title = {{Example of the Baum-Welch Algorithm}},
url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS{\&}CSC=Y{\&}NEWS=N{\&}PAGE=fulltext{\&}D=psyc3{\&}AN=1999-16082-004},
volume = {33},
year = {2008}
}
@article{Baker2013,
author = {Baker, Kirk},
file = {:C$\backslash$:/Users/vigne/Documents/Artificial Intelligence/Thesis/Recommender Systems Journals/Singular{\_}Value{\_}Decomposition{\_}Tutorial.pdf:pdf},
journal = {{\ldots}Value Decomposition Tutorial. pdf},
pages = {1--24},
title = {{Singular value decomposition tutorial}},
url = {http://lsa-svd-application-for-analysis.googlecode.com/svn-history/r120/trunk/LSA/Other/LsaToRead/SVDTut.pdf},
volume = {2005},
year = {2013}
}
